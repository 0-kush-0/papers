{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3387366,"sourceType":"datasetVersion","datasetId":2042151},{"sourceId":3763578,"sourceType":"datasetVersion","datasetId":2248331}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gdown\nfrom torch.nn import DataParallel\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_id = '1D1OxhivWv53ay0xrdF4CHuuywMgiFEmq'\noutput_file = 'output.zip'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdown.download(f'https://drive.google.com/uc?id={file_id}', output_file, quiet=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\n\nwith zipfile.ZipFile('output.zip', 'r') as zip_ref:\n    zip_ref.extractall('./data')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport logging\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nfrom torch.utils.data import DataLoader, random_split\nimport os\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nfrom torch import optim\nimport logging\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"execution":{"iopub.status.busy":"2023-12-23T07:50:41.261964Z","iopub.execute_input":"2023-12-23T07:50:41.262974Z","iopub.status.idle":"2023-12-23T07:50:46.800243Z","shell.execute_reply.started":"2023-12-23T07:50:41.262934Z","shell.execute_reply":"2023-12-23T07:50:46.798759Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"logging.basicConfig(format = \"%(asctime)s - %(levelname)s: %(message)s\", level = logging.INFO, datefmt = \"%I:%M:%S\")","metadata":{"execution":{"iopub.status.busy":"2023-12-23T07:50:46.803524Z","iopub.execute_input":"2023-12-23T07:50:46.804939Z","iopub.status.idle":"2023-12-23T07:50:46.811466Z","shell.execute_reply.started":"2023-12-23T07:50:46.804886Z","shell.execute_reply":"2023-12-23T07:50:46.810266Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class DiffusionForImages:\n    def __init__(self, sBeta = 1e-4, eBeta=0.02, timesteps=1000, imgsize = 256, device = \"cuda\"):\n        self.sBeta= sBeta\n        self.eBeta = eBeta\n        self.timesteps = timesteps\n        self.imgsize = imgsize\n        self.device = device\n        \n        self.beta = self.noise_scheduler().to(device)\n        self.alpha = 1. - self.beta\n        self.alpha_hat = torch.cumprod(self.alpha, dim = 0)\n\n    def noise_scheduler(self):\n        return torch.linspace(self.sBeta, self.eBeta, self.timesteps)\n\n    def noising_images(self, x, t):\n        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n\n        eta = torch.randn_like(x)\n\n        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * eta, eta\n\n\n    def get_timesteps(self, n):\n        return torch.randint(low=1, high = self.timesteps, size=(n,))\n\n    def sample_image(self, model, n):\n        logging.info(f\"Sampling {n} new images....\")\n        model.eval()\n        with torch.no_grad():\n            x = torch.randn((n,3,self.imgsize, self.imgsize)).to(self.device)\n            for i in tqdm(reversed(range(1, self.timesteps)), position = 0):\n                t = (torch.ones(n)*i).long().to(self.device)\n                predicted_noise = model(x,t)\n                alpha = self.alpha[t][:, None, None, None]\n                alpha_hat = self.alpha_hat[t][:, None,None,None]\n                beta = self.beta[t][:, None, None, None]\n\n                if i>1:\n                    noise = torch.randn_like(x)\n\n                else:\n                    noise = torch.zeros_like(x)\n\n                x = 1/torch.sqrt(alpha) * (x - ((1-alpha)/(torch.sqrt(1-alpha_hat)))*predicted_noise) + torch.sqrt(beta)*noise\n\n        model.train()\n        x = (x.clamp(-1,1)+1)/2\n        x = (x*255).type(torch.uint8)\n\n        return x\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T07:50:46.813724Z","iopub.execute_input":"2023-12-23T07:50:46.814309Z","iopub.status.idle":"2023-12-23T07:50:46.829293Z","shell.execute_reply.started":"2023-12-23T07:50:46.814281Z","shell.execute_reply":"2023-12-23T07:50:46.828245Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#different modules\nclass SelfAttention(nn.Module):\n    def __init__(self, channels, size):\n        super(SelfAttention, self).__init__()\n        self.channels = channels\n        self.size = size\n        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n        self.ln = nn.LayerNorm([channels])\n        self.ff_self = nn.Sequential(\n            nn.LayerNorm([channels]),\n            nn.Linear(channels, channels),\n            nn.GELU(),\n            nn.Linear(channels, channels),\n        )\n\n    def forward(self, x):\n        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n        x_ln = self.ln(x)\n        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n        attention_value = attention_value + x\n        attention_value = self.ff_self(attention_value) + attention_value\n        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n        super().__init__()\n        self.residual = residual\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n            nn.GroupNorm(1, mid_channels),\n            nn.GELU(),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.GroupNorm(1, out_channels),\n        )\n\n    def forward(self, x):\n        if self.residual:\n            return F.gelu(x + self.double_conv(x))\n        else:\n            return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    def __init__(self, in_channels, out_channels, emb_dim=256):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, in_channels, residual=True),\n            DoubleConv(in_channels, out_channels),\n        )\n\n        self.emb_layer = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(\n                emb_dim,\n                out_channels\n            ),\n        )\n\n    def forward(self, x, t):\n        x = self.maxpool_conv(x)\n        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n        # print(\"x= \",x.shape, \"t=\", t.shape)\n        return x + emb\n\n\nclass Up(nn.Module):\n    def __init__(self, in_channels, out_channels, emb_dim=256):\n        super().__init__()\n\n        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n        self.conv = nn.Sequential(\n            DoubleConv(in_channels, in_channels, residual=True),\n            DoubleConv(in_channels, out_channels, in_channels // 2),\n        )\n\n        self.emb_layer = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(\n                emb_dim,\n                out_channels\n            ),\n        )\n\n    def forward(self, x, skip_x, t):\n        x = self.up(x)\n        x = torch.cat([skip_x, x], dim=1)\n        x = self.conv(x)\n        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n        return x + emb\n\n\nclass UNet(nn.Module):\n    def __init__(self, c_in=3, c_out=3, time_dim=256, device=\"cuda\"):\n        super().__init__()\n        self.device = device\n        self.time_dim = time_dim\n        self.inc = DoubleConv(c_in, 64)\n        self.down1 = Down(64, 128)\n        self.sa1 = SelfAttention(128, 32)\n        self.down2 = Down(128, 256)\n        self.sa2 = SelfAttention(256, 16)\n        self.down3 = Down(256, 256)\n        self.sa3 = SelfAttention(256, 8)\n\n        self.bot1 = DoubleConv(256, 512)\n        self.bot2 = DoubleConv(512, 512)\n        self.bot3 = DoubleConv(512, 256)\n\n        self.up1 = Up(512, 128)\n        self.sa4 = SelfAttention(128, 16)\n        self.up2 = Up(256, 64)\n        self.sa5 = SelfAttention(64, 32)\n        self.up3 = Up(128, 64)\n        self.sa6 = SelfAttention(64, 64)\n        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n\n    def pos_encoding(self, t, channels):\n        inv_freq = 1.0 / (\n            10000\n            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n        )\n        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n        return pos_enc\n    \n\n    def forward(self, x, t):\n        t = t.unsqueeze(-1).type(torch.float)\n        t = self.pos_encoding(t, self.time_dim)\n\n        #encoder (performs downsampling)\n        x1 = self.inc(x)\n        x2 = self.down1(x1, t)\n        x2 = self.sa1(x2)\n        x3 = self.down2(x2, t)\n        x3 = self.sa2(x3)\n        x4 = self.down3(x3, t)\n        x4 = self.sa3(x4)\n\n        #bottleneck\n        x4 = self.bot1(x4)\n        x4 = self.bot2(x4)\n        x4 = self.bot3(x4)\n\n        #decoder (performs upsampling)\n        x = self.up1(x4, x3, t)\n        x = self.sa4(x)\n        x = self.up2(x, x2, t)\n        x = self.sa5(x)\n        x = self.up3(x, x1, t)\n        x = self.sa6(x)\n        output = self.outc(x)\n        return output\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T07:50:46.831516Z","iopub.execute_input":"2023-12-23T07:50:46.831891Z","iopub.status.idle":"2023-12-23T07:50:46.865767Z","shell.execute_reply.started":"2023-12-23T07:50:46.831863Z","shell.execute_reply":"2023-12-23T07:50:46.864886Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_images(images, ncols=4):\n    n_images = len(images)\n    nrows = (n_images + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 10))\n\n    for i, image in enumerate(images):\n        row = i // ncols\n        col = i % ncols\n\n        ax = axes[row, col]\n        ax.imshow(image.permute(1, 2, 0).cpu())\n        ax.axis('off')\n\n    # Hide any empty subplots\n    for i in range(n_images, nrows * ncols):\n        row = i // ncols\n        col = i % ncols\n        axes[row, col].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n\n\ndef save_images(images, path, **kwargs):\n    grid = torchvision.utils.make_grid(images, **kwargs)\n    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n    im = Image.fromarray(ndarr)\n    im.save(path)\n\n\ndef get_data(args):\n    # Define your transformations\n    transforms = torchvision.transforms.Compose([\n        torchvision.transforms.Resize(80),\n        torchvision.transforms.RandomResizedCrop(args.image_size, scale=(0.8, 1.0)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    # Load the entire dataset\n    full_dataset = torchvision.datasets.ImageFolder(args.dataset_path, transform=transforms)\n\n    # Choose the number of images you want to use (e.g., 10,000)\n    num_images_to_use = 10000\n\n    # Create a random subset of the dataset with the desired number of images\n    subset_dataset, _ = random_split(full_dataset, [num_images_to_use, len(full_dataset) - num_images_to_use])\n\n    # Create a dataloader for the subset dataset\n    dataloader = DataLoader(subset_dataset, batch_size=args.batch_size, shuffle=True)\n\n    return dataloader\n\ndef setup_logging(run_name):\n    os.makedirs(\"models\", exist_ok=True)\n    os.makedirs(\"results\", exist_ok=True)\n    os.makedirs(os.path.join(\"models\", run_name), exist_ok=True)\n    os.makedirs(os.path.join(\"results\", run_name), exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T07:50:46.867190Z","iopub.execute_input":"2023-12-23T07:50:46.867581Z","iopub.status.idle":"2023-12-23T07:50:46.890901Z","shell.execute_reply.started":"2023-12-23T07:50:46.867545Z","shell.execute_reply":"2023-12-23T07:50:46.889918Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from torch import optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nfrom torch.nn import DataParallel\n\ndef train(args):\n    setup_logging(args.run_name)\n    device = args.device\n    model = UNet()\n    \n    model = DataParallel(model)\n#     checkpoint_path = \"./models/DDPM/ckpt.pt\"\n#     model.load_state_dict(torch.load(checkpoint_path))\n    model = model.to(device)\n    dataloader = get_data(args)\n\n    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n    mse = nn.MSELoss()\n    diffusion = DiffusionForImages(imgsize=args.image_size, device=device)\n    logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\n    l = len(dataloader)\n    \n    count = 0\n    \n    for epoch in range(args.epochs):\n        count+=1\n        logging.info(f\"Starting epoch {epoch}:\")\n        pbar = tqdm(dataloader)\n        for i, (images, _) in enumerate(pbar):\n            images = images.to(device)\n            # print(f\"Batch {i} is on GPU {images.device.index}\")\n            \n            t = diffusion.get_timesteps(images.shape[0]).to(device)\n#             print(\"t= \",t)\n            x_t, noise = diffusion.noising_images(images, t)\n            # print(\"x_t= \", x_t.shape)\n            predicted_noise = model(x_t, t)\n            loss = mse(noise, predicted_noise)\n            # print(\"Outside: input size\", images.size())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            pbar.set_postfix(MSE=loss.item())\n            logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n\n        sampled_images = diffusion.sample_image(model, n=images.shape[0])\n#         save_images(sampled_images, os.path.join(\"results\", args.run_name, f\"{epoch+276}.jpg\"))\n        if count%5==0:\n            torch.save(model.state_dict(), os.path.join(\"models\", args.run_name, f\"ckpt.pt\"))\n            save_images(sampled_images, os.path.join(\"results\", args.run_name, f\"{epoch+276}.jpg\"))","metadata":{"execution":{"iopub.status.busy":"2023-12-23T07:59:39.958672Z","iopub.execute_input":"2023-12-23T07:59:39.959639Z","iopub.status.idle":"2023-12-23T07:59:39.972513Z","shell.execute_reply.started":"2023-12-23T07:59:39.959602Z","shell.execute_reply":"2023-12-23T07:59:39.971349Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def begin():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-f')\n    args = parser.parse_args()\n    args.run_name = \"DDPM\"\n    args.epochs = 500\n    args.batch_size = 17\n    args.image_size = 64\n    args.dataset_path = \"/kaggle/input/casia-webface/casia-webface\"\n    args.device = torch.device(\"cuda\")\n    args.lr = 3e-4\n    train(args)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T07:50:46.909465Z","iopub.execute_input":"2023-12-23T07:50:46.909780Z","iopub.status.idle":"2023-12-23T07:50:46.926910Z","shell.execute_reply.started":"2023-12-23T07:50:46.909755Z","shell.execute_reply":"2023-12-23T07:50:46.925966Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# import os\n# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"","metadata":{"execution":{"iopub.status.busy":"2023-12-23T07:50:46.928005Z","iopub.execute_input":"2023-12-23T07:50:46.928339Z","iopub.status.idle":"2023-12-23T07:50:46.941823Z","shell.execute_reply.started":"2023-12-23T07:50:46.928312Z","shell.execute_reply":"2023-12-23T07:50:46.941080Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    begin()","metadata":{"execution":{"iopub.status.busy":"2023-12-23T07:59:45.698468Z","iopub.execute_input":"2023-12-23T07:59:45.699287Z","iopub.status.idle":"2023-12-23T17:59:11.613362Z","shell.execute_reply.started":"2023-12-23T07:59:45.699254Z","shell.execute_reply":"2023-12-23T17:59:11.608471Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 589/589 [04:58<00:00,  1.97it/s, MSE=0.0178]\n999it [00:50, 19.69it/s]\n100%|██████████| 589/589 [04:00<00:00,  2.45it/s, MSE=0.0152]\n999it [00:50, 19.72it/s]\n100%|██████████| 589/589 [04:07<00:00,  2.38it/s, MSE=0.0145] \n999it [00:52, 18.99it/s]\n100%|██████████| 589/589 [04:07<00:00,  2.38it/s, MSE=0.00656]\n999it [00:53, 18.82it/s]\n100%|██████████| 589/589 [04:03<00:00,  2.41it/s, MSE=0.0135] \n999it [00:50, 19.96it/s]\n100%|██████████| 589/589 [04:03<00:00,  2.42it/s, MSE=0.0178] \n999it [00:50, 19.70it/s]\n100%|██████████| 589/589 [04:01<00:00,  2.44it/s, MSE=0.0705] \n999it [00:50, 19.97it/s]\n100%|██████████| 589/589 [04:01<00:00,  2.44it/s, MSE=0.0885] \n999it [00:50, 19.80it/s]\n100%|██████████| 589/589 [04:00<00:00,  2.45it/s, MSE=0.0739] \n999it [00:50, 19.60it/s]\n100%|██████████| 589/589 [04:00<00:00,  2.44it/s, MSE=0.0033] \n999it [00:49, 20.01it/s]\n100%|██████████| 589/589 [04:02<00:00,  2.43it/s, MSE=0.0109] \n999it [00:50, 19.80it/s]\n100%|██████████| 589/589 [04:00<00:00,  2.45it/s, MSE=0.039]  \n999it [00:50, 19.61it/s]\n100%|██████████| 589/589 [04:03<00:00,  2.42it/s, MSE=0.00103]\n999it [00:50, 19.70it/s]\n100%|██████████| 589/589 [04:01<00:00,  2.44it/s, MSE=0.0119] \n999it [00:51, 19.54it/s]\n100%|██████████| 589/589 [04:02<00:00,  2.43it/s, MSE=0.0112] \n999it [00:50, 19.66it/s]\n100%|██████████| 589/589 [04:02<00:00,  2.43it/s, MSE=0.00657]\n999it [00:50, 19.67it/s]\n100%|██████████| 589/589 [04:02<00:00,  2.43it/s, MSE=0.0138] \n999it [00:50, 19.81it/s]\n100%|██████████| 589/589 [04:03<00:00,  2.42it/s, MSE=0.00928]\n999it [00:53, 18.75it/s]\n100%|██████████| 589/589 [04:10<00:00,  2.35it/s, MSE=0.00844]\n999it [00:56, 17.53it/s]\n100%|██████████| 589/589 [04:13<00:00,  2.33it/s, MSE=0.0414] \n999it [00:55, 17.88it/s]\n100%|██████████| 589/589 [04:03<00:00,  2.42it/s, MSE=0.0673] \n999it [00:52, 19.09it/s]\n100%|██████████| 589/589 [04:31<00:00,  2.17it/s, MSE=0.000909]\n999it [00:51, 19.23it/s]\n100%|██████████| 589/589 [04:35<00:00,  2.14it/s, MSE=0.0185] \n999it [00:50, 19.74it/s]\n100%|██████████| 589/589 [04:03<00:00,  2.42it/s, MSE=0.00845]\n999it [00:50, 19.66it/s]\n100%|██████████| 589/589 [04:03<00:00,  2.42it/s, MSE=0.0034] \n999it [00:50, 19.88it/s]\n100%|██████████| 589/589 [04:02<00:00,  2.43it/s, MSE=0.00633]\n999it [00:50, 19.73it/s]\n100%|██████████| 589/589 [04:02<00:00,  2.43it/s, MSE=0.0127] \n999it [00:50, 19.92it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.00364]\n999it [00:50, 19.84it/s]\n100%|██████████| 589/589 [04:11<00:00,  2.34it/s, MSE=0.0225] \n999it [00:50, 19.87it/s]\n100%|██████████| 589/589 [04:11<00:00,  2.34it/s, MSE=0.00692]\n999it [00:50, 19.78it/s]\n100%|██████████| 589/589 [04:08<00:00,  2.37it/s, MSE=0.00741]\n999it [00:52, 19.10it/s]\n100%|██████████| 589/589 [04:10<00:00,  2.35it/s, MSE=0.0273] \n999it [00:51, 19.51it/s]\n100%|██████████| 589/589 [04:05<00:00,  2.40it/s, MSE=0.00772]\n999it [00:51, 19.44it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.0108] \n999it [00:51, 19.40it/s]\n100%|██████████| 589/589 [04:17<00:00,  2.29it/s, MSE=0.0209] \n999it [00:53, 18.81it/s]\n100%|██████████| 589/589 [04:12<00:00,  2.34it/s, MSE=0.00508]\n999it [00:54, 18.36it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.0102] \n999it [00:53, 18.51it/s]\n100%|██████████| 589/589 [04:05<00:00,  2.40it/s, MSE=0.00293]\n999it [00:54, 18.44it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.0173] \n999it [00:54, 18.38it/s]\n100%|██████████| 589/589 [04:07<00:00,  2.38it/s, MSE=0.0246] \n999it [00:53, 18.84it/s]\n100%|██████████| 589/589 [04:07<00:00,  2.38it/s, MSE=0.0636] \n999it [00:53, 18.54it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.00936]\n999it [00:53, 18.83it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.00301]\n999it [00:53, 18.60it/s]\n100%|██████████| 589/589 [04:04<00:00,  2.41it/s, MSE=0.0048] \n999it [00:52, 18.86it/s]\n100%|██████████| 589/589 [04:03<00:00,  2.41it/s, MSE=0.0323] \n999it [00:53, 18.66it/s]\n100%|██████████| 589/589 [04:04<00:00,  2.41it/s, MSE=0.0138] \n999it [00:52, 18.92it/s]\n100%|██████████| 589/589 [04:05<00:00,  2.40it/s, MSE=0.0025] \n999it [00:53, 18.82it/s]\n100%|██████████| 589/589 [04:05<00:00,  2.40it/s, MSE=0.00618]\n999it [00:53, 18.61it/s]\n100%|██████████| 589/589 [04:07<00:00,  2.38it/s, MSE=0.00997]\n999it [00:52, 18.87it/s]\n100%|██████████| 589/589 [04:05<00:00,  2.40it/s, MSE=0.00163]\n999it [00:53, 18.84it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.000865]\n999it [00:52, 19.01it/s]\n100%|██████████| 589/589 [04:05<00:00,  2.40it/s, MSE=0.0165] \n999it [00:53, 18.79it/s]\n100%|██████████| 589/589 [04:07<00:00,  2.38it/s, MSE=0.00636]\n999it [00:53, 18.75it/s]\n100%|██████████| 589/589 [04:09<00:00,  2.36it/s, MSE=0.0011] \n999it [00:52, 19.07it/s]\n100%|██████████| 589/589 [04:04<00:00,  2.41it/s, MSE=0.0253] \n999it [00:53, 18.75it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.0515] \n999it [00:53, 18.81it/s]\n100%|██████████| 589/589 [04:09<00:00,  2.36it/s, MSE=0.0125] \n999it [00:52, 18.86it/s]\n100%|██████████| 589/589 [04:09<00:00,  2.36it/s, MSE=0.0164] \n999it [00:53, 18.63it/s]\n100%|██████████| 589/589 [04:07<00:00,  2.38it/s, MSE=0.00684]\n999it [00:55, 18.10it/s]\n100%|██████████| 589/589 [04:05<00:00,  2.39it/s, MSE=0.0447] \n999it [00:52, 18.91it/s]\n100%|██████████| 589/589 [04:08<00:00,  2.37it/s, MSE=0.00215]\n999it [00:52, 19.06it/s]\n100%|██████████| 589/589 [04:05<00:00,  2.40it/s, MSE=0.00116]\n999it [00:53, 18.78it/s]\n100%|██████████| 589/589 [04:05<00:00,  2.40it/s, MSE=0.0109] \n999it [00:53, 18.76it/s]\n100%|██████████| 589/589 [04:09<00:00,  2.36it/s, MSE=0.0208] \n999it [00:52, 18.92it/s]\n100%|██████████| 589/589 [04:09<00:00,  2.36it/s, MSE=0.0179] \n999it [00:53, 18.78it/s]\n100%|██████████| 589/589 [04:10<00:00,  2.35it/s, MSE=0.00898]\n999it [00:52, 18.89it/s]\n100%|██████████| 589/589 [04:09<00:00,  2.36it/s, MSE=0.0462] \n999it [00:53, 18.77it/s]\n100%|██████████| 589/589 [04:11<00:00,  2.34it/s, MSE=0.0104] \n100%|██████████| 589/589 [04:09<00:00,  2.36it/s, MSE=0.0328] \n999it [00:52, 18.85it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.0471] \n999it [00:53, 18.80it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.00112]\n999it [00:52, 18.91it/s]\n100%|██████████| 589/589 [04:09<00:00,  2.36it/s, MSE=0.00946]\n999it [00:53, 18.64it/s]\n100%|██████████| 589/589 [04:05<00:00,  2.40it/s, MSE=0.00255]\n999it [00:53, 18.51it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.0151] \n999it [00:52, 18.93it/s]\n100%|██████████| 589/589 [04:10<00:00,  2.36it/s, MSE=0.0114] \n999it [00:53, 18.79it/s]\n100%|██████████| 589/589 [04:08<00:00,  2.37it/s, MSE=0.0205] \n999it [00:52, 18.95it/s]\n100%|██████████| 589/589 [04:03<00:00,  2.42it/s, MSE=0.00825]\n999it [00:53, 18.81it/s]\n100%|██████████| 589/589 [04:03<00:00,  2.41it/s, MSE=0.0301] \n999it [00:53, 18.62it/s]\n100%|██████████| 589/589 [04:10<00:00,  2.35it/s, MSE=0.00477]\n999it [00:52, 18.99it/s]\n100%|██████████| 589/589 [04:11<00:00,  2.34it/s, MSE=0.0149] \n999it [00:53, 18.77it/s]\n100%|██████████| 589/589 [04:12<00:00,  2.33it/s, MSE=0.0163] \n999it [00:52, 18.95it/s]\n100%|██████████| 589/589 [04:12<00:00,  2.33it/s, MSE=0.00213]\n999it [00:53, 18.69it/s]\n100%|██████████| 589/589 [04:13<00:00,  2.32it/s, MSE=0.0236] \n999it [00:53, 18.69it/s]\n100%|██████████| 589/589 [04:10<00:00,  2.35it/s, MSE=0.0789] \n999it [00:52, 18.92it/s]\n100%|██████████| 589/589 [04:07<00:00,  2.38it/s, MSE=0.00242]\n999it [00:53, 18.73it/s]\n100%|██████████| 589/589 [04:04<00:00,  2.41it/s, MSE=0.0256] \n999it [00:52, 18.96it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.0227] \n999it [00:52, 18.88it/s]\n100%|██████████| 589/589 [04:05<00:00,  2.40it/s, MSE=0.0623] \n999it [00:53, 18.63it/s]\n100%|██████████| 589/589 [04:04<00:00,  2.41it/s, MSE=0.0082] \n999it [00:52, 18.88it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.00336]\n999it [00:52, 18.87it/s]\n100%|██████████| 589/589 [04:07<00:00,  2.38it/s, MSE=0.013]  \n999it [00:52, 18.85it/s]\n100%|██████████| 589/589 [04:14<00:00,  2.32it/s, MSE=0.00366]\n999it [00:53, 18.75it/s]\n100%|██████████| 589/589 [04:13<00:00,  2.32it/s, MSE=0.021]  \n999it [00:53, 18.78it/s]\n100%|██████████| 589/589 [04:10<00:00,  2.35it/s, MSE=0.0375] \n999it [00:49, 20.08it/s]\n100%|██████████| 589/589 [04:01<00:00,  2.44it/s, MSE=0.034]  \n999it [00:50, 19.85it/s]\n100%|██████████| 589/589 [03:58<00:00,  2.47it/s, MSE=0.00311]\n999it [00:49, 20.01it/s]\n100%|██████████| 589/589 [04:01<00:00,  2.44it/s, MSE=0.0283] \n999it [00:50, 19.88it/s]\n100%|██████████| 589/589 [04:04<00:00,  2.41it/s, MSE=0.0104] \n999it [00:50, 19.69it/s]\n100%|██████████| 589/589 [04:01<00:00,  2.44it/s, MSE=0.00608]\n999it [00:49, 20.08it/s]\n100%|██████████| 589/589 [04:01<00:00,  2.44it/s, MSE=0.0101] \n999it [00:49, 20.07it/s]\n100%|██████████| 589/589 [04:05<00:00,  2.40it/s, MSE=0.0155] \n999it [00:49, 19.99it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.0122] \n999it [00:49, 19.98it/s]\n100%|██████████| 589/589 [04:02<00:00,  2.43it/s, MSE=0.0127] \n999it [00:50, 19.91it/s]\n100%|██████████| 589/589 [04:01<00:00,  2.43it/s, MSE=0.0048] \n999it [00:49, 20.00it/s]\n999it [00:51, 19.41it/s] [03:39<00:23,  2.41it/s, MSE=0.0095] \n100%|██████████| 589/589 [04:00<00:00,  2.44it/s, MSE=0.0196] \n999it [00:52, 19.19it/s]\n100%|██████████| 589/589 [04:03<00:00,  2.42it/s, MSE=0.00591]\n999it [00:52, 19.13it/s]\n100%|██████████| 589/589 [04:04<00:00,  2.41it/s, MSE=0.00968]\n999it [00:52, 18.90it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.00931]\n999it [00:51, 19.28it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.00828]\n999it [00:52, 19.20it/s]\n100%|██████████| 589/589 [04:05<00:00,  2.40it/s, MSE=0.023]  \n999it [00:51, 19.22it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.00139]\n999it [00:51, 19.21it/s]\n100%|██████████| 589/589 [04:05<00:00,  2.40it/s, MSE=0.023]  \n999it [00:52, 18.99it/s]\n100%|██████████| 589/589 [04:06<00:00,  2.39it/s, MSE=0.00353]\n999it [00:51, 19.22it/s]\n100%|██████████| 589/589 [04:02<00:00,  2.42it/s, MSE=0.0191] \n999it [00:50, 19.96it/s]\n100%|██████████| 589/589 [04:00<00:00,  2.45it/s, MSE=0.0113] \n999it [00:49, 20.07it/s]\n100%|██████████| 589/589 [04:01<00:00,  2.44it/s, MSE=0.00812]\n999it [00:50, 19.90it/s]\n100%|██████████| 589/589 [03:59<00:00,  2.46it/s, MSE=0.0112] \n999it [00:50, 19.63it/s]\n100%|██████████| 589/589 [04:01<00:00,  2.43it/s, MSE=0.00164]\n999it [00:49, 19.98it/s]\n100%|██████████| 589/589 [04:03<00:00,  2.42it/s, MSE=0.00129]\n999it [00:50, 19.91it/s]\n 22%|██▏       | 130/589 [00:53<03:08,  2.43it/s, MSE=0.0222] \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mbegin\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m args\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m args\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3e-4\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[10], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     42\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 45\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_postfix(MSE\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     46\u001b[0m     logger\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem(), global_step\u001b[38;5;241m=\u001b[39mepoch \u001b[38;5;241m*\u001b[39m l \u001b[38;5;241m+\u001b[39m i)\n\u001b[1;32m     48\u001b[0m sampled_images \u001b[38;5;241m=\u001b[39m diffusion\u001b[38;5;241m.\u001b[39msample_image(model, n\u001b[38;5;241m=\u001b[39mimages\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"device = \"cuda\"\n\n# Load the checkpt \ncheckpoint = torch.load('./models/DDPM/ckpt.pt')\n\n# Remove the 'module.' prefix from keys if present (for single GPU)\nstate_dict = {k.replace('module.', ''): v for k, v in checkpoint.items()}\n\n\nmodel = UNet().to(device)  \nmodel.load_state_dict(state_dict)\n\n\ndiffusion = DiffusionForImages(imgsize=64, device=device)\nx = diffusion.sample_image(model, n=16)\nplot_images(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    device = \"cuda\"\n    model = UNet()\n    model = DataParallel(model)\n    ckpt = torch.load(\"./models/DDPM/ckpt.pt\")\n    model.load_state_dict(ckpt)\n    model = model.to(device)\n    diffusion = DiffusionForImages(imgsize=64, device=device)\n    x = diffusion.sample_image(model, n=64)\n    print(x.shape)\n    plot_images(x)\n    save_images(x, os.path.join(\"results\", \"DDPM\", \"a.jpg\"))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torchvision\n# from torchvision import transforms\n# from PIL import Image\n\n# # Load pre-trained WideResNet50 model\n# model = torchvision.models.wide_resnet50_2(pretrained=True)\n# model.eval()\n\n# # Transformation for resizing images to 224x224\n# transform = transforms.Compose([\n#     transforms.Resize((256, 256)),\n#     transforms.ToTensor(),\n# ])\n\n# transform2 = transforms.Compose([\n#     transforms.Resize((512, 512ed)),\n#     transforms.ToTensor(),\n# ])\n\n# # Example RGB image (assuming img is your input image)\n# # Replace this with your own image data\n# img = Image.open('/kaggle/input/casia-webface/casia-webface/000000/00000001.jpg')\n\n\n# # Transform the image\n# input_image = transform(img).unsqueeze(0)  # Add a batch dimension\n# v=input_image.shape\n# print(v)\n\n# # Access intermediate layers for feature extraction\n# features = list(model.children())[:-3]  # Get layers excluding the last two (avgpool and fc)\n# feature_extractor = torch.nn.Sequential(*features)\n\n# # Extract features from the original image\n# with torch.no_grad():\n#     original_features = feature_extractor(input_image)\n#     original_features = original_features.squeeze(dim=0)\n\n# # Resize and split the RGB image into 4 non-overlapping patches\n# patches = []  # Store the 4 patches\n# patch_size = (224, 224)\n# width, height = img.size\n# for i in range(2):\n#     for j in range(2):\n#         left = j * patch_size[0]\n#         upper = i * patch_size[1]\n#         right = left + patch_size[0]\n#         lower = upper + patch_size[1]\n#         patch = img.crop((left, upper, right, lower))\n#         patches.append(transform(patch).unsqueeze(0))  # Add each patch to the list\n\n# # Extract features for each patch\n# patch_features = []\n# with torch.no_grad():\n#     for patch in patches:\n#         patch_feature = feature_extractor(patch)\n# #         patch_feature = patch_feature.squeeze(dim=0)\n#         patch_features.append(patch_feature)\n\n# Feature maps for the original image (size: 1024 x 14 x 14)\nprint(f\"Feature maps for the original image: {original_features.shape}\")\n\n# feature_cat = torch.cat(original_features, dim=0)\n# feature_cat.shape\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torchvision\n# from torchvision import transforms\n# from PIL import Image\n\n# # Load pre-trained WideResNet50 model\n# model = torchvision.models.wide_resnet50_2(pretrained=True)\n# model.eval()\n\n# # Transformation for resizing images to 224x224\n# transform = transforms.Compose([\n#     transforms.Resize((256, 256)),\n#     transforms.ToTensor(),\n# ])\n\n# transform2 = transforms.Compose([\n#     transforms.Resize((512, 512)),\n# #     transforms.ToTensor(),\n# ])\n\n# tensor = transforms.Compose([\n#     transforms.ToTensor(),\n# ])\n\n\n# features = list(model.children())[:-3]  # Get layers excluding the last two (avgpool and fc)\n# feature_extractor = torch.nn.Sequential(*features)\n\n# patch_size = (256, 256)\n\n\n# dataset_path = '/kaggle/input/casia-webface/casia-webface'\n\n# for image_file in os.listdir(dataset_path):\n#     image_pth = os.path.join(dataset_path, image_file)\n\n#     for image in os.listdir(image_pth):\n#         image_path = os.path.join(image_pth, image)\n#         img = Image.open(image_path)\n#         width, height = img.size\n#         input_image = transform(img).unsqueeze(0)\n        \n#         with torch.no_grad():\n#             f1 = feature_extractor(input_image)\n# #             f1 = f1.squeeze(dim=0)\n            \n#         #scale2 feature extraction\n#         img2 = transform2(img)\n        \n#         patches=[]\n#         patch_features=[]\n#         for i in range(2):\n#             for j in range(2):\n#                 left = j * patch_size[0]\n#                 upper = i * patch_size[1]\n#                 right = left + patch_size[0]\n#                 lower = upper + patch_size[1]\n#                 patch = img2.crop((left, upper, right, lower))\n#                 patches.append(tensor(patch).unsqueeze(0))\n        \n#         with torch.no_grad():\n#             for patch in patches:\n#                 patch_feature = feature_extractor(patch)\n#                 patch_features.append(patch_feature)\n#         patch_features.append(f1)\n#         final_feature = torch.cat(patch_features, dim=0)\n        \n#         #storing the feature tensor\n#         output_folder = f'/kaggle/working/{image_file}/'\n\n#         # Check if the folder exists, and create it if not\n#         if not os.path.exists(output_folder):\n#             os.makedirs(output_folder)\n#         file_path = os.path.join(output_folder, f'{image}.pt')\n\n#         # Open the file and save the tensor\n#         with open(file_path, 'wb') as file:\n#             torch.save(final_feature, file)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nds = '/kaggle/input/casia-webface'\ndataloader = DataLoader(ds, batch_size=32, shuffle=True)\n\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\n\n# Load pre-trained WideResNet50 model\nmodel1 = torchvision.models.wide_resnet50_2(pretrained=True)\nmodel1.eval()\n\n# Transformation for resizing images to 224x224\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n])\n\ntransform2 = transforms.Compose([\n    transforms.Resize((512, 512)),\n#     transforms.ToTensor(),\n])\n\ntensor = transforms.Compose([\n    transforms.ToTensor(),\n])\n\n\nfeatures_layer = list(model1.children())[:-3]  # Get layers excluding the last two (avgpool and fc)\nfeature_extractor = torch.nn.Sequential(*features_layer)\n\npatch_size = (256, 256)\n\n\n\ndef feature_function(img):\n    \n    width, height = img.size\n    input_image = transform(img).unsqueeze(0)\n\n    with torch.no_grad():\n        f1 = feature_extractor(input_image)\n#             f1 = f1.squeeze(dim=0)\n\n    #scale2 feature extraction\n    img2 = transform2(img)\n\n    patches=[]\n    patch_features=[]\n    for i in range(2):\n        for j in range(2):\n            left = j * patch_size[0]\n            upper = i * patch_size[1]\n            right = left + patch_size[0]\n            lower = upper + patch_size[1]\n            patch = img2.crop((left, upper, right, lower))\n            patches.append(tensor(patch).unsqueeze(0))\n\n    with torch.no_grad():\n        for patch in patches:\n            patch_feature = feature_extractor(patch)\n            patch_features.append(patch_feature)\n    patch_features.append(f1)\n    final_feature = torch.cat(patch_features, dim=0)\n    return final_feature\n        \n    \ndef train_branch_2(args):\n    setup_logging(args.run_name)\n    device = args.device\n    model = UNet()\n    \n    model = DataParallel(model)\n    checkpoint_path = \"./models/DDPM/ckpt.pt\"\n    model.load_state_dict(torch.load(checkpoint_path))\n    model = model.to(device)\n    dataloader = get_data(args)\n\n    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n    mse = nn.MSELoss()\n    diffusion = DiffusionForImages(imgsize=args.image_size, device=device)\n    logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\n    l = len(dataloader)\n    \n    count = 0\n    \n    for epoch in range(args.epochs):\n        count+=1\n        logging.info(f\"Starting epoch {epoch}:\")\n        pbar = tqdm(dataloader)\n        for i, (images, _) in enumerate(pbar):\n            images = images.to(device)\n            # print(f\"Batch {i} is on GPU {images.device.index}\")\n            batch_features = []\n\n            # Perform feature extraction on-the-fly for each image in the batch\n            for image in images:\n                feature = feature_function(image)\n                batch_features.append(feature)\n\n            # Convert the list of features to a tensor\n            features = torch.stack(batch_features)\n            \n            t = diffusion.get_timesteps(features.shape[0]).to(device)\n#             print(\"t= \",t)\n            x_t, noise = diffusion.noising_images(features, t)\n            # print(\"x_t= \", x_t.shape)\n            predicted_noise = model(x_t, t)\n            loss = mse(noise, predicted_noise)\n            # print(\"Outside: input size\", images.size())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            pbar.set_postfix(MSE=loss.item())\n            logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n\n        sampled_images = diffusion.sample_image(model, n=features.shape[0])\n#         save_images(sampled_images, os.path.join(\"results\", args.run_name, f\"{epoch+276}.jpg\"))\n        torch.save(model.state_dict(), os.path.join(\"models\", args.run_name, f\"ckpt.pt\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_scale_1(model):\n    model.eval()\n\n    # Transformation for resizing images to 224x224\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n    ])\n    \n\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/gdrive')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras_retinanet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_resnet import models\nfrom keras.layers import Input\n\n# Define input tensor shape\ninput_shape = (800, 800, 3)  # Adjust the shape according to your input images\n\n# Create an input tensor\ninputs = Input(shape=input_shape)\n\n# Initialize ResNet2D50 model with the input tensor\nmodel = models.ResNet2D50(inputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np\nfrom keras_retinanet import models\nfrom keras_retinanet.utils.image import preprocess_image, resize_image\n\nconfidence_threshold = 0.5\n\n# Load the pre-trained RetinaNet model\nmodel = models.load_model('/kaggle/input/object-detection-model/resnet50_coco_best_v2.1.0.h5')\n\n# Path to CASIA-WebFace images\ndataset_path = '/kaggle/input/casia-webface/casia-webface'\n\n# Iterate through images\nfor image_file in os.listdir(dataset_path):\n    image_pth = os.path.join(dataset_path, image_file)\n\n    for image in os.listdir(image_pth):\n        image_path = os.path.join(image_pth, image)\n        # Load and preprocess the image\n        imge = cv2.imread(image_path)\n        imge = preprocess_image(imge)\n        imge, scale = resize_image(imge)\n\n        # Use the model to predict faces\n        boxes, scores, labels = model.predict(np.expand_dims(imge, axis=0))\n\n        # Filter out low confidence detections\n        high_confidence_indices = np.where(scores[0] > confidence_threshold)[0]\n        print(high_confidence_indices)\n        boxes = boxes[0, high_confidence_indices]\n        print(\"boxes\",boxes)\n        scores = scores[0, high_confidence_indices]\n        print(\"scores\",scores)\n\n        # Extract and save faces\n        for box in boxes:\n            x, y, x2, y2 = box.astype(int)\n            face = imge[y:y2, x:x2]\n            print(\"in\")\n            output_folder = f'/kaggle/working/{image_file}/'\n\n            # Check if the folder exists, and create it if not\n            if not os.path.exists(output_folder):\n                os.makedirs(output_folder)\n            cv2.imwrite(os.path.join(output_folder, f'{image}.jpg'), face)\n            print(\"out\")\n\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}